{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine  with Squared Hinge Loss function - Real Data example\n",
    "\n",
    "Elham Rezvani\n",
    "SML-Spring 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load necessary packages\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.linalg import eigh as largest_eigh\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.debugger import Tracer\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and standardize the data, and convert y to + 1 and -1 \n",
    "\n",
    "spam = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data', sep=' ', header=None) \n",
    "test_indicator = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.traintest', sep=' ',header=None)\n",
    "spam = np.array(spam)\n",
    "spam.shape\n",
    "x = spam[:,0:-1]\n",
    "y = spam[:,-1]\n",
    "std_scaleX = StandardScaler().fit(x)\n",
    "x = std_scaleX.transform(x)\n",
    "y = y *2 -1\n",
    "\n",
    "#Separate data into test set and train set, where each set has the same proportion of spam and not spam\n",
    "\n",
    "def splitTestTrain(x, y, testPCT):\n",
    "    # x is an array of features\n",
    "    # y is an array of response variables\n",
    "    # testPCT is the percent of the dataset that you would like to be included in the test set. The remainder is in the training set\n",
    "    # Output: A test set including features and response, and a training set inclduing features and response. T\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=testPCT, random_state=0)\n",
    "    for train_index, test_index in sss.split(x,y):\n",
    "        xTrain, xTest = x[train_index], x[test_index]\n",
    "        yTrain, yTest = y[train_index], y[test_index]\n",
    "        return (xTrain, xTest,yTrain, yTest)\n",
    "    \n",
    "xTrain, xTest,yTrain, yTest = splitTestTrain(x,y,.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below functions implement the following functions:**\n",
    "  \n",
    "Objective function of the squared hinge loss function:\n",
    "$$ F(\\beta) = \\frac{1}{n} \\sum_{i=1}^n(max(0,1-y_ix_i^T\\beta))^2 + \\lambda||\\beta||_2^2$$\n",
    "\n",
    "Gradient of the squared hinge loss function:\n",
    "$$ \\nabla F(\\beta)=\\frac{-2}{n} \\sum_{i=1}^n\\bigl(y_ix_i  * max(0,1−y_ix_i^Tβ)\\bigr) + 2\\lambda\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement SVM with squared hinge loss we need to develop a fast gradient descent algorithm. \n",
    "# Our fast gradient descent algorithm relies on several input functions including:\n",
    "# - computegrad: computes the gradient of the squared hinge loss function at a given point beta\n",
    "# - objfunc: computes the objective function\n",
    "# - backtracking function: computes the step size in each iteration of fast gradient descent algorithm\n",
    "\n",
    "#Initialize Gradient Calculation function\n",
    "def computegrad(betas, lam, x, y):\n",
    "    #betas: point at which gradient calculation occurs\n",
    "    #lam: lambda paramater, as defined by user\n",
    "    #x: feature vector\n",
    "    #y: response vector\n",
    "    #output: gradient\n",
    "    Maxes = np.maximum(np.zeros(y.shape[0]),(1- (y*np.dot(x,betas))))\n",
    "    grad = np.sum((x.T*y)*Maxes, axis = 1)\n",
    "    grad = -2* grad/x.shape[0] + 2 * lam * betas\n",
    "    return(grad)\n",
    "\n",
    "def objfunc(betas, lam, x, y):\n",
    "    #betas: point at which evalution of objective function occurs\n",
    "    #lam: lambda paramater, as defined by user\n",
    "    #x: feature vector\n",
    "    #y: response vector\n",
    "    #output: value of objective function\n",
    "    Maxes = np.maximum(np.zeros(y.shape[0]),(1- (y*np.dot(x,betas))))\n",
    "    obj = sum(Maxes**2)/x.shape[0] + lam * np.linalg.norm(betas)**2\n",
    "    return(obj)\n",
    "\n",
    "def backtracking( betas,lam,x, y, t, alpha = 0.5, gamma = 0.8, maxIter = 100):\n",
    "    # betas = current point\n",
    "    # lam  = lambda parameter \n",
    "    # x = array of feature data\n",
    "    # y = array of response data\n",
    "    # t = starting step size\n",
    "    # alpha = constant used to define sufficient decrease condition\n",
    "    # gamma = fraction by which we drecrease t if the previous T doesn't work\n",
    "    # maxIter = maximum iterations for algorithm\n",
    "    #output = t, the step size to use\n",
    "\n",
    "    grad_b = computegrad(betas = betas, lam = lam , x=x, y=y)\n",
    "    norm_grad_b = np.linalg.norm(grad_b)\n",
    "    found_t = 0\n",
    "    iters = 0\n",
    "    t = deepcopy(t)\n",
    "    while found_t == 0 and iters < maxIter:\n",
    "        if objfunc(betas = (betas-t*grad_b), lam = lam, x=x, y=y) < objfunc(betas = betas, lam = lam , x=x , y=y)-alpha*t*norm_grad_b**2:\n",
    "            found_t = 1 \n",
    "        else:\n",
    "            t *= gamma\n",
    "            iters += 1\n",
    "    return(t)\n",
    "\n",
    "def mylinearsvm(betas, lam, x,  y,  alpha = .5, gamma = .5, maxIter = 200):\n",
    "    # betas = current point\n",
    "    # lam  = lambda parameter \n",
    "    # x = array of feature data\n",
    "    # y = array of response data\n",
    "    # alpha = constant used to define sufficient decrease condition within the backtracking function\n",
    "    # gamma = fraction by which we drecrease t if the previous T doesn't work, within the backtracking function\n",
    "    # maxIter = maximum iterations for algorithm\n",
    "    # output = a vector of betas that corresponds to the minimum value of the objective function\n",
    "    theta = np.zeros(xTrain.shape[1])\n",
    "    b_vals = [betas]\n",
    "    n = x.shape[0]\n",
    "    n1 =  x.shape[1]\n",
    "    \n",
    "    #Calculate initial step size\n",
    "    MaxEigVal = largest_eigh(np.dot(1/n*x.T, x), eigvals= (n1-1, n1-1))[0]\n",
    "    StepInit = 1/(MaxEigVal + lam)\n",
    "    \n",
    "    for i in range(0,maxIter):\n",
    "        step = backtracking(betas = betas, t = deepcopy(StepInit), lam = lam, x = x, y = y, alpha = alpha, gamma = gamma)\n",
    "        betaNew = theta - step*computegrad(betas = theta, lam = lam, x = x, y = y)\n",
    "        b_vals.append(betaNew) \n",
    "        theta = betaNew+ (i/(i+3))*(betaNew - betas)\n",
    "        betas = deepcopy(betaNew)\n",
    "    return(b_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This set of functions relates to checking and using the output of SVM with squared hinge loss. Funcions include:\n",
    "# ViewGradDescent: to view gradient descent to determine whether it was successful in finding convergence. \n",
    "# class_error: to find the classification error that results from the model and a given dataset\n",
    "# crossValid: for a given array of lambdas, finds the lowest misclassification error and the lambda it is associated with\n",
    "# makePreds: for a given model and test set, make predictions\n",
    "def viewGradDescent(betas, lam, x, y):\n",
    "    # betas = array of betas created by mySVM function \n",
    "    # lam = lambda used in creation of betas above\n",
    "    # x = x array used in creation of betas above\n",
    "    # y = y array used in creation of betas above\n",
    "    #output = plot of objective value of squared hinge loss function given parameters above. Used to demonstrat whether or not SVM converged\n",
    "    objVals = []\n",
    "    for i in range(0,len(betas)):\n",
    "        objVals.append(objfunc(betas = betas[i], lam = lam, x = x, y = y))\n",
    "    plt.plot(range(len(objVals)), objVals, label = \"mySVM\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    return\n",
    "\n",
    "def class_error (betaStar, x , y): \n",
    "    #Inputs:\n",
    "    # betaStar = betas corresponding to fitted model (last row of betas created by mysvm function)\n",
    "    # x = array of features you wish to find misclassification error\n",
    "    # y = array of response you wish to find misclassification error\n",
    "    #Output: Caluclation of percent of predictions for which the model was incorrect\n",
    "    pred = (-x.dot(betaStar)) > 0 \n",
    "    pred = pred*2- 1 # Convert to +/− 1 \n",
    "    error = 1- np.mean(pred !=y)\n",
    "    return error\n",
    "\n",
    "def crossValid(xTrain, yTrain, xTest, yTest, lambdas):\n",
    "    #Inputs:\n",
    "    # xTrain and yTrain = array of features and responses you wish to use to fit the model\n",
    "    # xTest and yTest = array of features you wish to use to test the model\n",
    "    # lambdas: array of lambdas you wish to use    #Output: Caluclation of percent of predictions for which the model was incorrect\n",
    "    # outputs:\n",
    "    #     array of errors associatd with lambdas\n",
    "    #     the value of lambda associated with the lowest misclassification error\n",
    "    #     the value of the lowest misclassification error\n",
    "    \n",
    "    #Initialize variables\n",
    "    betaTrack = (np.zeros(xTrain.shape[1]))\n",
    "    errorTrack = np.zeros(len(lambdas))\n",
    "   \n",
    "    #Fit model for given value of lambda using training data \n",
    "    for i in lamdas:\n",
    "        Betas = mylinearsvm(betas = np.zeros(xTrain.shape[1]), x=xTrain,  y=yTrain, lam = i, maxIter = 100)[-1]\n",
    "        betaTrack = np.vstack((betaTrack, Betas))\n",
    "    betaTrack = betaTrack[1:]\n",
    "   \n",
    "    #Caluclate predictions and errors for test data\n",
    "    for i in range(0,len(betaTrack)):\n",
    "            pred = 1/(1+np.exp(-xTest.dot(betaTrack[i]))) > 0.5 \n",
    "            pred = pred*2- 1 # Convert to +/− 1 \n",
    "            errorTrack[i] = np.mean(pred !=yTest)\n",
    "    \n",
    "    # Plot errors and return best value of lambda and corresponding error\n",
    "    bestLam = lamdas[np.argmin(errorTrack,0)]\n",
    "    minError = np.min(errorTrack)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(lamdas,errorTrack, 'r-')\n",
    "    plt.xlabel('Lambda')\n",
    "    plt.ylabel('Misclassification Error')\n",
    "    ax.plot()\n",
    "    plt.xscale('log')\n",
    "    plt.title('Misclassification Error vs Lambda')\n",
    "    return errorTrack, bestLam, minError\n",
    "\n",
    "def makePreds(x, betas):\n",
    "    #inputs:\n",
    "    #Features that you want to make predictions for\n",
    "    #betas =  model that you want to use to make predictions\n",
    "    #output: prediction for each of n rows in your xTest set\n",
    "    pred = 1/(1+np.exp(-x.dot(betas))) > 0.5 \n",
    "    pred = pred*2- 1 # Convert to +/− 1 \n",
    "    return(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lowest error found using cross validation is 0.0738488271069 and it corresponds to a lambda of 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEaCAYAAAAsQ0GGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG9IlY46rIe4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25b99cb4080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run above functions to find the lowest misclassification error and it's corresponding lambda\n",
    "\n",
    "lamdas = np.array([.00001, .0001, .001, .01, .1, 1, 10, 100, 1000, 10000])\n",
    "Errors, BestLam, LowestError = crossValid(xTrain, yTrain, xTest, yTest, lambdas = lamdas)\n",
    "print('The Lowest error found using cross validation is', LowestError,'and it corresponds to a lambda of', BestLam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The misclassifciation error on the training set is: 0.0866666666667\n",
      "The misclassifciation error on the test set is: 0.0729800173762\n"
     ]
    }
   ],
   "source": [
    "#Fit a model using our best lambdas\n",
    "betas = mylinearsvm(betas = np.zeros(xTrain.shape[1]), x= xTrain,  y= yTrain, lam = BestLam, maxIter = 1000)\n",
    "betas[-1]\n",
    "#calculate misclassification error assocated with model, and make predictions\n",
    "Train_error = class_error(betas[-1], xTrain, yTrain)\n",
    "Test_error = class_error(betas[-1], xTest, yTest)\n",
    "print(\"The misclassifciation error on the training set is:\", Train_error)\n",
    "print(\"The misclassifciation error on the test set is:\", Test_error)\n",
    "mySVMPreds = makePreds(xTest, betas[-1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
