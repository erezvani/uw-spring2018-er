{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine  with Squared Hinge Loss function -- Simulated Data Example\n",
    "\n",
    "Elham Rezvani\n",
    "SML-spring2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load necessary packages\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.linalg import eigh as largest_eigh\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.debugger import Tracer\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate simulated data\n",
    "\n",
    "vars = 50\n",
    "seeds = np.arange(0,100,2)\n",
    "def genSims(seeds = seeds, n = 100, p=vars, cats = 2):\n",
    "    mySim = np.empty(shape = (n*cats,p))\n",
    "    for i in range(0,p):\n",
    "        temp = np.array(np.random.normal(loc = seeds[i], scale = seeds[i], size = n))\n",
    "        temp = np.append(temp, np.random.normal(loc = seeds[i]*1.5, scale = seeds[i]*1.3, size = n)) \n",
    "        mySim[:,i] = temp\n",
    "    return(mySim)\n",
    "data = genSims()\n",
    "x = data\n",
    "y = np.append(np.zeros(100)+1, np.zeros(100)-1)\n",
    "std_scaleX = StandardScaler().fit(x)\n",
    "x = std_scaleX.transform(x)\n",
    "\n",
    "#Separate data into test set and train set, where each set has the same proportion of spam and not spam\n",
    "\n",
    "def splitTestTrain(x, y, testPCT):\n",
    "    # x is an array of features\n",
    "    # y is an array of response variables\n",
    "    # testPCT is the percent of the dataset that you would like to be included in the test set. The remainder is in the training set\n",
    "    # Output: A test set including features and response, and a training set inclduing features and response. T\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=testPCT, random_state=0)\n",
    "    for train_index, test_index in sss.split(x,y):\n",
    "        xTrain, xTest = x[train_index], x[test_index]\n",
    "        yTrain, yTest = y[train_index], y[test_index]\n",
    "        return (xTrain, xTest,yTrain, yTest)\n",
    "    \n",
    "xTrain, xTest,yTrain, yTest = splitTestTrain(x,y,.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below functions implement the following functions:**\n",
    "  \n",
    "Objective function of the squared hinge loss function:\n",
    "$$ F(\\beta) = \\frac{1}{n} \\sum_{i=1}^n(max(0,1-y_ix_i^T\\beta))^2 + \\lambda||\\beta||_2^2$$\n",
    "\n",
    "Gradient of the squared hinge loss function:\n",
    "$$ \\nabla F(\\beta)=\\frac{-2}{n} \\sum_{i=1}^n\\bigl(y_ix_i  * max(0,1−y_ix_i^Tβ)\\bigr) + 2\\lambda\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement SVM with squared hinge loss we need to develop a fast gradient descent algorithm. \n",
    "# Our fast gradient descent algorithm relies on several input functions including:\n",
    "# - computegrad: computes the gradient of the squared hinge loss function at a given point beta\n",
    "# - objfunc: computes the objective function\n",
    "# - backtracking function: computes the step size in each iteration of fast gradient descent algorithm\n",
    "\n",
    "#Initialize Gradient Calculation function\n",
    "def computegrad(betas, lam, x, y):\n",
    "    #betas: point at which gradient calculation occurs\n",
    "    #lam: lambda paramater, as defined by user\n",
    "    #x: feature vector\n",
    "    #y: response vector\n",
    "    #output: gradient\n",
    "    Maxes = np.maximum(np.zeros(y.shape[0]),(1- (y*np.dot(x,betas))))\n",
    "    grad = np.sum((x.T*y)*Maxes, axis = 1)\n",
    "    grad = -2* grad/x.shape[0] + 2 * lam * betas\n",
    "    return(grad)\n",
    "\n",
    "def objfunc(betas, lam, x, y):\n",
    "    #betas: point at which evalution of objective function occurs\n",
    "    #lam: lambda paramater, as defined by user\n",
    "    #x: feature vector\n",
    "    #y: response vector\n",
    "    #output: value of objective function\n",
    "    Maxes = np.maximum(np.zeros(y.shape[0]),(1- (y*np.dot(x,betas))))\n",
    "    obj = sum(Maxes**2)/x.shape[0] + lam * np.linalg.norm(betas)**2\n",
    "    return(obj)\n",
    "\n",
    "def backtracking( betas,lam,x, y, t, alpha = 0.5, gamma = 0.8, maxIter = 100):\n",
    "    # betas = current point\n",
    "    # lam  = lambda parameter \n",
    "    # x = array of feature data\n",
    "    # y = array of response data\n",
    "    # t = starting step size\n",
    "    # alpha = constant used to define sufficient decrease condition\n",
    "    # gamma = fraction by which we drecrease t if the previous T doesn't work\n",
    "    # maxIter = maximum iterations for algorithm\n",
    "    #output = t, the step size to use\n",
    "\n",
    "    grad_b = computegrad(betas = betas, lam = lam , x=x, y=y)\n",
    "    norm_grad_b = np.linalg.norm(grad_b)\n",
    "    found_t = 0\n",
    "    iters = 0\n",
    "    t = deepcopy(t)\n",
    "    while found_t == 0 and iters < maxIter:\n",
    "        if objfunc(betas = (betas-t*grad_b), lam = lam, x=x, y=y) < objfunc(betas = betas, lam = lam , x=x , y=y)-alpha*t*norm_grad_b**2:\n",
    "            found_t = 1 \n",
    "        else:\n",
    "            t *= gamma\n",
    "            iters += 1\n",
    "    return(t)\n",
    "\n",
    "def mylinearsvm(betas, lam, x,  y,  alpha = .5, gamma = .5, maxIter = 200):\n",
    "    # betas = current point\n",
    "    # lam  = lambda parameter \n",
    "    # x = array of feature data\n",
    "    # y = array of response data\n",
    "    # alpha = constant used to define sufficient decrease condition within the backtracking function\n",
    "    # gamma = fraction by which we drecrease t if the previous T doesn't work, within the backtracking function\n",
    "    # maxIter = maximum iterations for algorithm\n",
    "    # output = a vector of betas that corresponds to the minimum value of the objective function\n",
    "    theta = np.zeros(xTrain.shape[1])\n",
    "    b_vals = [betas]\n",
    "    n = x.shape[0]\n",
    "    n1 =  x.shape[1]\n",
    "    \n",
    "    #Calculate initial step size\n",
    "    MaxEigVal = largest_eigh(np.dot(1/n*x.T, x), eigvals= (n1-1, n1-1))[0]\n",
    "    StepInit = 1/(MaxEigVal + lam)\n",
    "    \n",
    "    for i in range(0,maxIter):\n",
    "        step = backtracking(betas = betas, t = deepcopy(StepInit), lam = lam, x = x, y = y, alpha = alpha, gamma = gamma)\n",
    "        betaNew = theta - step*computegrad(betas = theta, lam = lam, x = x, y = y)\n",
    "        b_vals.append(betaNew) \n",
    "        theta = betaNew+ (i/(i+3))*(betaNew - betas)\n",
    "        betas = deepcopy(betaNew)\n",
    "    return(b_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bullet 4: ** Train your linear support vector machine with the squared hinge loss on the the Spam\n",
    "dataset for the λ = 1. Report your misclassification error for this value of λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This set of functions relates to checking and using the output of SVM with squared hinge loss. Funcions include:\n",
    "# ViewGradDescent: to view gradient descent to determine whether it was successful in finding convergence. \n",
    "# class_error: to find the classification error that results from the model and a given dataset\n",
    "# crossValid: for a given array of lambdas, finds the lowest misclassification error and the lambda it is associated with\n",
    "# makePreds: for a given model and test set, make predictions\n",
    "def viewGradDescent(betas, lam, x, y):\n",
    "    # betas = array of betas created by mySVM function \n",
    "    # lam = lambda used in creation of betas above\n",
    "    # x = x array used in creation of betas above\n",
    "    # y = y array used in creation of betas above\n",
    "    #output = plot of objective value of squared hinge loss function given parameters above. Used to demonstrat whether or not SVM converged\n",
    "    objVals = []\n",
    "    for i in range(0,len(betas)):\n",
    "        objVals.append(objfunc(betas = betas[i], lam = lam, x = x, y = y))\n",
    "    plt.plot(range(len(objVals)), objVals, label = \"mySVM\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    return\n",
    "\n",
    "def class_error (betaStar, x , y): \n",
    "    #Inputs:\n",
    "    # betaStar = betas corresponding to fitted model (last row of betas created by mysvm function)\n",
    "    # x = array of features you wish to find misclassification error\n",
    "    # y = array of response you wish to find misclassification error\n",
    "    #Output: Caluclation of percent of predictions for which the model was incorrect\n",
    "    pred = (-x.dot(betaStar)) > 0 \n",
    "    pred = pred*2- 1 # Convert to +/− 1 \n",
    "    error = 1- np.mean(pred !=y)\n",
    "    return error\n",
    "\n",
    "def crossValid(xTrain, yTrain, xTest, yTest, lambdas):\n",
    "    #Inputs:\n",
    "    # xTrain and yTrain = array of features and responses you wish to use to fit the model\n",
    "    # xTest and yTest = array of features you wish to use to test the model\n",
    "    # lambdas: array of lambdas you wish to use    #Output: Caluclation of percent of predictions for which the model was incorrect\n",
    "    # outputs:\n",
    "    #     array of errors associatd with lambdas\n",
    "    #     the value of lambda associated with the lowest misclassification error\n",
    "    #     the value of the lowest misclassification error\n",
    "    \n",
    "    #Initialize variables\n",
    "    betaTrack = (np.zeros(xTrain.shape[1]))\n",
    "    errorTrack = np.zeros(len(lambdas))\n",
    "   \n",
    "    #Fit model for given value of lambda using training data \n",
    "    for i in lamdas:\n",
    "        Betas = mylinearsvm(betas = np.zeros(xTrain.shape[1]), x=xTrain,  y=yTrain, lam = i, maxIter = 100)[-1]\n",
    "        betaTrack = np.vstack((betaTrack, Betas))\n",
    "    betaTrack = betaTrack[1:]\n",
    "   \n",
    "    #Caluclate predictions and errors for test data\n",
    "    for i in range(0,len(betaTrack)):\n",
    "            pred = 1/(1+np.exp(-xTest.dot(betaTrack[i]))) > 0.5 \n",
    "            pred = pred*2- 1 # Convert to +/− 1 \n",
    "            errorTrack[i] = np.mean(pred !=yTest)\n",
    "    \n",
    "    # Plot errors and return best value of lambda and corresponding error\n",
    "    bestLam = lamdas[np.argmin(errorTrack,0)]\n",
    "    minError = np.min(errorTrack)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(lamdas,errorTrack, 'r-')\n",
    "    plt.xlabel('Lambda')\n",
    "    plt.ylabel('Misclassification Error')\n",
    "    ax.plot()\n",
    "    plt.xscale('log')\n",
    "    plt.title('Misclassification Error vs Lambda')\n",
    "    return errorTrack, bestLam, minError\n",
    "\n",
    "def makePreds(x, betas):\n",
    "    #inputs:\n",
    "    #Features that you want to make predictions for\n",
    "    #betas =  model that you want to use to make predictions\n",
    "    #output: prediction for each of n rows in your xTest set\n",
    "    pred = 1/(1+np.exp(-x.dot(betas))) > 0.5 \n",
    "    pred = pred*2- 1 # Convert to +/− 1 \n",
    "    return(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lowest error found using cross validation is 0.08 and it corresponds to a lambda of 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nufx/U2GS1IkTAbwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23a30b049e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run above functions to find the lowest misclassification error and it's corresponding lambda\n",
    "\n",
    "lamdas = np.array([.00001, .0001, .001, .01, .1, 1, 10, 100, 1000, 10000])\n",
    "Errors, BestLam, LowestError = crossValid(xTrain, yTrain, xTest, yTest, lambdas = lamdas)\n",
    "print('The Lowest error found using cross validation is', LowestError,'and it corresponds to a lambda of', BestLam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The misclassifciation error on the training set is: 0.0733333333333\n",
      "The misclassifciation error on the test set is: 0.08\n"
     ]
    }
   ],
   "source": [
    "#Fit a model using our best lambdas\n",
    "betas = mylinearsvm(betas = np.zeros(xTrain.shape[1]), x= xTrain,  y= yTrain, lam = BestLam, maxIter = 1000)\n",
    "betas[-1]\n",
    "#calculate misclassification error assocated with model, and make predictions\n",
    "Train_error = class_error(betas[-1], xTrain, yTrain)\n",
    "Test_error = class_error(betas[-1], xTest, yTest)\n",
    "print(\"The misclassifciation error on the training set is:\", Train_error)\n",
    "print(\"The misclassifciation error on the test set is:\", Test_error)\n",
    "mySVMPreds = makePreds(xTest, betas[-1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
