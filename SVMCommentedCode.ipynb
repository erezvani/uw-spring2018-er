{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine with Squared Hinge Loss function - commented code\n",
    "\n",
    "Elham Rezvani\n",
    "SML-Spring2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load necessary packages\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.linalg import eigh as largest_eigh\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.debugger import Tracer\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ca33a5f3dac7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitTestTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Separate data into test set and train set, where each set has the same proportion of spam and not spam\n",
    "\n",
    "def splitTestTrain(x, y, testPCT):\n",
    "    # x is an array of features\n",
    "    # y is an array of response variables\n",
    "    # testPCT is the percent of the dataset that you would like to be included in the test set. The remainder is in the training set\n",
    "    # Output: A test set including features and response, and a training set inclduing features and response. T\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=testPCT, random_state=0)\n",
    "    for train_index, test_index in sss.split(x,y):\n",
    "        xTrain, xTest = x[train_index], x[test_index]\n",
    "        yTrain, yTest = y[train_index], y[test_index]\n",
    "        return (xTrain, xTest,yTrain, yTest)\n",
    "    \n",
    "xTrain, xTest,yTrain, yTest = splitTestTrain(x,y,.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement SVM with squared hinge loss we need to develop a fast gradient descent algorithm. \n",
    "# Our fast gradient descent algorithm relies on several input functions including:\n",
    "# - computegrad: computes the gradient of the squared hinge loss function at a given point beta\n",
    "# - objfunc: computes the objective function\n",
    "# - backtracking function: computes the step size in each iteration of fast gradient descent algorithm\n",
    "\n",
    "#Initialize Gradient Calculation function\n",
    "def computegrad(betas, lam, x, y):\n",
    "    #betas: point at which gradient calculation occurs\n",
    "    #lam: lambda paramater, as defined by user\n",
    "    #x: feature vector\n",
    "    #y: response vector\n",
    "    #output: gradient\n",
    "    Maxes = np.maximum(np.zeros(y.shape[0]),(1- (y*np.dot(x,betas))))\n",
    "    grad = np.sum((x.T*y)*Maxes, axis = 1)\n",
    "    grad = -2* grad/x.shape[0] + 2 * lam * betas\n",
    "    return(grad)\n",
    "\n",
    "def objfunc(betas, lam, x, y):\n",
    "    #betas: point at which evalution of objective function occurs\n",
    "    #lam: lambda paramater, as defined by user\n",
    "    #x: feature vector\n",
    "    #y: response vector\n",
    "    #output: value of objective function\n",
    "    Maxes = np.maximum(np.zeros(y.shape[0]),(1- (y*np.dot(x,betas))))\n",
    "    obj = sum(Maxes**2)/x.shape[0] + lam * np.linalg.norm(betas)**2\n",
    "    return(obj)\n",
    "\n",
    "def backtracking( betas,lam,x, y, t, alpha = 0.5, gamma = 0.8, maxIter = 100):\n",
    "    # betas = current point\n",
    "    # lam  = lambda parameter \n",
    "    # x = array of feature data\n",
    "    # y = array of response data\n",
    "    # t = starting step size\n",
    "    # alpha = constant used to define sufficient decrease condition\n",
    "    # gamma = fraction by which we drecrease t if the previous T doesn't work\n",
    "    # maxIter = maximum iterations for algorithm\n",
    "    #output = t, the step size to use\n",
    "\n",
    "    grad_b = computegrad(betas = betas, lam = lam , x=x, y=y)\n",
    "    norm_grad_b = np.linalg.norm(grad_b)\n",
    "    found_t = 0\n",
    "    iters = 0\n",
    "    t = deepcopy(t)\n",
    "    while found_t == 0 and iters < maxIter:\n",
    "        if objfunc(betas = (betas-t*grad_b), lam = lam, x=x, y=y) < objfunc(betas = betas, lam = lam , x=x , y=y)-alpha*t*norm_grad_b**2:\n",
    "            found_t = 1 \n",
    "        else:\n",
    "            t *= gamma\n",
    "            iters += 1\n",
    "    return(t)\n",
    "\n",
    "def mylinearsvm(betas, lam, x,  y,  alpha = .5, gamma = .5, maxIter = 200):\n",
    "    # betas = current point\n",
    "    # lam  = lambda parameter \n",
    "    # x = array of feature data\n",
    "    # y = array of response data\n",
    "    # alpha = constant used to define sufficient decrease condition within the backtracking function\n",
    "    # gamma = fraction by which we drecrease t if the previous T doesn't work, within the backtracking function\n",
    "    # maxIter = maximum iterations for algorithm\n",
    "    # output = a vector of betas that corresponds to the minimum value of the objective function\n",
    "    theta = np.zeros(xTrain.shape[1])\n",
    "    b_vals = [betas]\n",
    "    n = x.shape[0]\n",
    "    n1 =  x.shape[1]\n",
    "    \n",
    "    #Calculate initial step size\n",
    "    MaxEigVal = largest_eigh(np.dot(1/n*x.T, x), eigvals= (n1-1, n1-1))[0]\n",
    "    StepInit = 1/(MaxEigVal + lam)\n",
    "    \n",
    "    for i in range(0,maxIter):\n",
    "        step = backtracking(betas = betas, t = deepcopy(StepInit), lam = lam, x = x, y = y, alpha = alpha, gamma = gamma)\n",
    "        betaNew = theta - step*computegrad(betas = theta, lam = lam, x = x, y = y)\n",
    "        b_vals.append(betaNew) \n",
    "        theta = betaNew+ (i/(i+3))*(betaNew - betas)\n",
    "        betas = deepcopy(betaNew)\n",
    "    return(b_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This set of functions relates to checking and using the output of SVM with squared hinge loss. Funcions include:\n",
    "# ViewGradDescent: to view gradient descent to determine whether it was successful in finding convergence. \n",
    "# class_error: to find the classification error that results from the model and a given dataset\n",
    "# crossValid: for a given array of lambdas, finds the lowest misclassification error and the lambda it is associated with\n",
    "# makePreds: for a given model and test set, make predictions\n",
    "def viewGradDescent(betas, lam, x, y):\n",
    "    # betas = array of betas created by mySVM function \n",
    "    # lam = lambda used in creation of betas above\n",
    "    # x = x array used in creation of betas above\n",
    "    # y = y array used in creation of betas above\n",
    "    #output = plot of objective value of squared hinge loss function given parameters above. Used to demonstrat whether or not SVM converged\n",
    "    objVals = []\n",
    "    for i in range(0,len(betas)):\n",
    "        objVals.append(objfunc(betas = betas[i], lam = lam, x = x, y = y))\n",
    "    plt.plot(range(len(objVals)), objVals, label = \"mySVM\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    return\n",
    "\n",
    "def class_error (betaStar, x , y): \n",
    "    #Inputs:\n",
    "    # betaStar = betas corresponding to fitted model (last row of betas created by mysvm function)\n",
    "    # x = array of features you wish to find misclassification error\n",
    "    # y = array of response you wish to find misclassification error\n",
    "    #Output: Caluclation of percent of predictions for which the model was incorrect\n",
    "    pred = (-x.dot(betaStar)) > 0 \n",
    "    pred = pred*2- 1 # Convert to +/âˆ’ 1 \n",
    "    error = 1- np.mean(pred !=y)\n",
    "    return error\n",
    "\n",
    "def crossValid(xTrain, yTrain, xTest, yTest, lambdas):\n",
    "    #Inputs:\n",
    "    # xTrain and yTrain = array of features and responses you wish to use to fit the model\n",
    "    # xTest and yTest = array of features you wish to use to test the model\n",
    "    # lambdas: array of lambdas you wish to use    #Output: Caluclation of percent of predictions for which the model was incorrect\n",
    "    # outputs:\n",
    "    #     array of errors associatd with lambdas\n",
    "    #     the value of lambda associated with the lowest misclassification error\n",
    "    #     the value of the lowest misclassification error\n",
    "    \n",
    "    #Initialize variables\n",
    "    betaTrack = (np.zeros(xTrain.shape[1]))\n",
    "    errorTrack = np.zeros(len(lambdas))\n",
    "   \n",
    "    #Fit model for given value of lambda using training data \n",
    "    for i in lamdas:\n",
    "        Betas = mylinearsvm(betas = np.zeros(xTrain.shape[1]), x=xTrain,  y=yTrain, lam = i, maxIter = 100)[-1]\n",
    "        betaTrack = np.vstack((betaTrack, Betas))\n",
    "    betaTrack = betaTrack[1:]\n",
    "   \n",
    "    #Caluclate predictions and errors for test data\n",
    "    for i in range(0,len(betaTrack)):\n",
    "            pred = 1/(1+np.exp(-xTest.dot(betaTrack[i]))) > 0.5 \n",
    "            pred = pred*2- 1 # Convert to +/âˆ’ 1 \n",
    "            errorTrack[i] = np.mean(pred !=yTest)\n",
    "    \n",
    "    # Plot errors and return best value of lambda and corresponding error\n",
    "    bestLam = lamdas[np.argmin(errorTrack,0)]\n",
    "    minError = np.min(errorTrack)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(lamdas,errorTrack, 'r-')\n",
    "    plt.xlabel('Lambda')\n",
    "    plt.ylabel('Misclassification Error')\n",
    "    ax.plot()\n",
    "    plt.xscale('log')\n",
    "    plt.title('Misclassification Error vs Lambda')\n",
    "    return errorTrack, bestLam, minError\n",
    "\n",
    "def makePreds(x, betas):\n",
    "    #inputs:\n",
    "    #Features that you want to make predictions for\n",
    "    #betas =  model that you want to use to make predictions\n",
    "    #output: prediction for each of n rows in your xTest set\n",
    "    pred = 1/(1+np.exp(-x.dot(betas))) > 0.5 \n",
    "    pred = pred*2- 1 # Convert to +/âˆ’ 1 \n",
    "    return(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust lambda to reflect the different objective function that Scikit-Learn uses\n",
    "def convertLam(lam):\n",
    "    convertedLam = 1/(2*xTrain.shape[0]*lam)\n",
    "    return(convertedLam)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
